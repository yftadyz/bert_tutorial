{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dCpvgG0vwXAZ"
   },
   "source": [
    "# Predicting Movie Review Sentiment with BERT on TF Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hsZvic2YxnTz"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime\n",
    "import time\n",
    "import gc\n",
    "\n",
    "import utils\n",
    "import tokenization as tz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pmFYvkylMwXn"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MC_w8SRqN0fr"
   },
   "source": [
    "First, let's download the dataset, hosted by Stanford. The code below, which downloads, extracts, and imports the IMDB Large Movie Review Dataset, is borrowed from [this Tensorflow tutorial](https://www.tensorflow.org/hub/tutorials/text_classification_with_tf_hub)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fom_ff20gyy6"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Load all files from a directory in a DataFrame.\n",
    "def load_directory_data(directory):\n",
    "    data = {}\n",
    "    data[\"sentence\"] = []\n",
    "    data[\"sentiment\"] = []\n",
    "    for file_path in os.listdir(directory):\n",
    "        with tf.compat.v1.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
    "            data[\"sentence\"].append(f.read())\n",
    "            data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
    "    return pd.DataFrame.from_dict(data)\n",
    "\n",
    "# Merge positive and negative examples, add a polarity column and shuffle.\n",
    "def load_dataset(directory):\n",
    "    pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
    "    neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
    "    pos_df[\"polarity\"] = 1\n",
    "    neg_df[\"polarity\"] = 0\n",
    "    return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Download and process the dataset files.\n",
    "def download_and_load_datasets(force_download=False):\n",
    "    dataset = tf.compat.v1.keras.utils.get_file(\n",
    "        fname=\"aclImdb.tar.gz\", \n",
    "        origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
    "        extract=True)\n",
    "  \n",
    "    train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                       \"aclImdb\", \"train\"))\n",
    "    test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                      \"aclImdb\", \"test\"))\n",
    "  \n",
    "    return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2abfwdn-g135"
   },
   "outputs": [],
   "source": [
    "train, test = download_and_load_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XA8WHJgzhIZf"
   },
   "source": [
    "To keep training fast, we'll take a sample of 5000 train and test examples, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lw_F488eixTV"
   },
   "outputs": [],
   "source": [
    "train = train.sample(5000)\n",
    "test = test.sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "prRQM8pDi8xI",
    "outputId": "e1c4f42f-5500-4b86-8641-ea21b5f7ea60"
   },
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sfRnHSz3iSXz"
   },
   "source": [
    "For us, our input data is the 'sentence' column and our label is the 'polarity' column (0, 1 for negative and positive, respecitvely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IuMOGwFui4it"
   },
   "outputs": [],
   "source": [
    "DATA_COLUMN = 'sentence'\n",
    "LABEL_COLUMN = 'polarity'\n",
    "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
    "label_list = [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V399W0rqNJ-Z"
   },
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "We'll need to transform our data into a format BERT understands. This involves two steps. First, we create  `InputExample`'s using the constructor provided in the BERT library.\n",
    "\n",
    "- `text_a` is the text we want to classify, which in this case, is the `Request` field in our Dataframe. \n",
    "- `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is `text_b` a translation of `text_a`? Is `text_b` an answer to the question asked by `text_a`?). This doesn't apply to our task, so we can leave `text_b` blank.\n",
    "- `label` is the label for our example, i.e. True, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9gEt5SmM6i6"
   },
   "outputs": [],
   "source": [
    "# Use the InputExample class from BERT's run_classifier code to create examples from the data( I copied that code and put it in util.py)\n",
    "\n",
    "train_InputExamples = train.apply(lambda x: utils.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "test_InputExamples = test.apply(lambda x: utils.InputExample(guid=None, \n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SCZWZtKxObjh"
   },
   "source": [
    "Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\n",
    "\n",
    "\n",
    "1. Lowercase our text (if we're using a BERT lowercase model)\n",
    "2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
    "3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
    "4. Map our words to indexes using a vocab file that BERT provides\n",
    "5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\n",
    "6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n",
    "\n",
    "Happily, we don't have to worry about most of these details.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qMWiDtpyQSoU"
   },
   "source": [
    "To start, we'll need to load a vocabulary file and build a tokenizer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IhJSe0QHNG7U"
   },
   "outputs": [],
   "source": [
    "tokenizer=tz.FullTokenizer(\n",
    "      vocab_file='vocab.txt', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z4oFkhpZBDKm"
   },
   "source": [
    "Great--we just learned that the BERT model we're using expects lowercase data and we also loaded BERT's vocab file. We also created a tokenizer, which breaks words into word pieces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "colab_type": "code",
    "id": "dsBo6RCtQmwx",
    "outputId": "20e2f343-f68c-4e4c-c66b-a1ca56c42cb6"
   },
   "outputs": [],
   "source": [
    "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0OEzfFIt6GIc"
   },
   "source": [
    "Using our tokenizer, we'll call `convert_examples_to_features` on our InputExamples to convert them into features BERT understands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "LL5W8gEGRTAf",
    "outputId": "6dc6de34-e14f-4739-d367-aeb31724aaf2"
   },
   "outputs": [],
   "source": [
    "# We'll set sequences to be at most 128 tokens long.\n",
    "MAX_SEQ_LENGTH = 128\n",
    "# Convert our train and test features to InputFeatures that BERT understands.\n",
    "train_features = utils.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "test_features = utils.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ccp5trMwRtmr"
   },
   "source": [
    "# Creating a model\n",
    "\n",
    "Now that we've prepared our data, let's focus on building a model. First, it loads the BERT tf hub module as a keras layer. Next, it creates a single new layer that will be trained to adapt BERT to our sentiment task (i.e. classifying whether a movie review is positive or negative). This strategy of using a mostly trained model is called [fine-tuning](http://wiki.fast.ai/index.php/Fine_tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "StCIZErUmzLz"
   },
   "outputs": [],
   "source": [
    "class BERT(tf.keras.Model):\n",
    "    def __init__(self,para=None):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        drop_rate=para['drop_rate']\n",
    "        \n",
    "        '''self.bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\",\n",
    "                            trainable=True,name='bert_layer')'''\n",
    "        \n",
    "        self.bert_layer = hub.KerasLayer(\"bert_en_uncased_L-12_H-768_A-12_2\",\n",
    "                            trainable=True,name='bert_layer')\n",
    "\n",
    "        self.dp_layer=tf.keras.layers.Dropout(drop_rate)\n",
    "        \n",
    "        self.task_output=tf.keras.layers.Dense(1,activation=tf.nn.sigmoid,name='task_specific_output_layer')\n",
    "        \n",
    "    def call(self,ft,training=False):\n",
    "        \n",
    "        pooled_output, sequence_output = self.bert_layer([ft['input_ids'], ft['input_mask'], ft['segment_ids']])\n",
    "        \n",
    "        pooled_output=self.dp_layer(pooled_output,training=training)\n",
    "        \n",
    "        return self.task_output(pooled_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "6o2a5ZIvRcJq",
    "outputId": "7b53001e-0e96-407e-bb78-93158d46bd4a"
   },
   "outputs": [],
   "source": [
    "# training\n",
    "para={}\n",
    "para['drop_rate']=0.1\n",
    "model=BERT(para)\n",
    "\n",
    "# build trainset from train_features\n",
    "input_params={}\n",
    "input_params['batch_size']=32\n",
    "train_input_fn=utils.input_fn_builder(train_features,MAX_SEQ_LENGTH,is_training=True,drop_remainder=False)\n",
    "trainset=train_input_fn(input_params)\n",
    "\n",
    "step=0\n",
    "epoch_num=3\n",
    "epoch_size=len(train_features)//input_params['batch_size']+1\n",
    "\n",
    "#set loss,optimizer and metric for training\n",
    "loss=tf.keras.losses.BinaryCrossentropy()\n",
    "opt=tf.keras.optimizers.Adam(learning_rate=0.00002)\n",
    "mt=tf.keras.metrics.AUC()\n",
    "\n",
    "loss_=[]\n",
    "mt_=[]\n",
    "ob=3\n",
    "loss_sum=0\n",
    "\n",
    "print('Start TRAINING...')\n",
    "now=time.time()\n",
    "currdate=time.strftime(\"%Y-%m-%d %H:%M:%S\",time.localtime(now))\n",
    "print(currdate)\n",
    "print(\"Epoch Size: %d\"%epoch_size)\n",
    "for i in range(epoch_num):\n",
    "    for ft in trainset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred=model(ft)\n",
    "            cur_loss=loss(ft['label_ids'],pred)\n",
    "        grads=tape.gradient(cur_loss,model.trainable_variables)\n",
    "        opt.apply_gradients(zip(grads,model.trainable_variables))\n",
    "        \n",
    "        loss_sum+=cur_loss\n",
    "        \n",
    "        if (step+1)%ob==0:\n",
    "            loss_.append(loss_sum/ob)\n",
    "            loss_sum=0\n",
    "            print(\"Step %d of Epoch %d......\"%(step,i))\n",
    "            print(\"Logloss: %.4f\"%loss_[-1])\n",
    "        \n",
    "        step+=1\n",
    "        \n",
    "        if step==epoch_size:\n",
    "            step=0\n",
    "            break\n",
    "    #save model weights of each epoch \n",
    "    model.save_weights('save/sw_epoch%d'%i)\n",
    "\n",
    "print('TRAINING is done!')\n",
    "now=time.time()\n",
    "currdate=time.strftime(\"%Y-%m-%d %H:%M:%S\",time.localtime(now))\n",
    "print(currdate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use our test data to see how well our model did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pJb7-GuKpdgD"
   },
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics. \n",
    "def metric_fn(label_ids, predicted_labels):\n",
    "    accuracy = tf.keras.metrics.BinaryAccuracy()\n",
    "    accuracy.update_state(label_ids, predicted_labels)\n",
    "    \n",
    "    auc = tf.keras.metrics.AUC()\n",
    "    auc.update_state(label_ids, predicted_labels)\n",
    "    \n",
    "    recall = tf.keras.metrics.Recall()\n",
    "    recall.update_state(label_ids, predicted_labels)\n",
    "    r=recall.result().numpy()\n",
    "    \n",
    "    precision = tf.keras.metrics.Precision()\n",
    "    precision.update_state(label_ids, predicted_labels)\n",
    "    p=precision.result().numpy()\n",
    "    \n",
    "    return {\n",
    "        \"eval_accuracy\": accuracy.result().numpy(),\n",
    "        \"auc\": auc.result().numpy(),\n",
    "        \"precision\": p,\n",
    "        \"recall\": r,\n",
    "        \"f1_score\": 2*p*r/(p+r)\n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "colab_type": "code",
    "id": "OwgKMYb6oVt3",
    "outputId": "7a4f108a-7952-4f53-b259-de66a110685d"
   },
   "outputs": [],
   "source": [
    "#build testset from test_features\n",
    "input_params={}\n",
    "input_params['batch_size']=32\n",
    "test_input_fn=utils.input_fn_builder(test_features,MAX_SEQ_LENGTH,is_training=False,drop_remainder=False)\n",
    "testset=test_input_fn(input_params)\n",
    "\n",
    "predicted_labels=[]\n",
    "label_ids=[]\n",
    "\n",
    "#make predictions on testset\n",
    "step=0\n",
    "for ft in testset:\n",
    "    if step%10==0:\n",
    "        print(\"Step %d ...\" % step)\n",
    "    predicted_labels+=model(ft).numpy().tolist()\n",
    "    label_ids+=ft['label_ids'].numpy().tolist()\n",
    "    step+=1\n",
    "\n",
    "#evaluation\n",
    "result=metric_fn(label_ids, predicted_labels)\n",
    "\n",
    "print(\"auc: %.4f\"%result['auc'])\n",
    "print(\"eval_accuracy: %.4f\"%result['eval_accuracy'])\n",
    "print(\"f1_score: %.4f\"%result['f1_score'])\n",
    "print(\"precision: %.4f\"%result['precision'])\n",
    "print(\"recall: %.4f\"%result['recall'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ueKsULteiz1B"
   },
   "source": [
    "Now let's write code to make predictions on new sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OsrbTD2EJTVl"
   },
   "outputs": [],
   "source": [
    "def getPrediction(in_sentences):\n",
    "    labels = [\"Negative\", \"Positive\"]\n",
    "    input_examples = [utils.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n",
    "    input_features = utils.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "    \n",
    "    predict_input_fn = utils.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
    "    input_params={}\n",
    "    input_params['batch_size']=32\n",
    "    newset=predict_input_fn(input_params)\n",
    "    \n",
    "    predicted_probs=[]\n",
    "    for ft in newset:\n",
    "        predicted_probs+=model(ft).numpy().squeeze().tolist()\n",
    "    return [(sentence, prediction, labels[1] if prediction>0.5 else labels[0]) for sentence, prediction in zip(in_sentences, predicted_probs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-thbodgih_VJ"
   },
   "outputs": [],
   "source": [
    "pred_sentences = [\n",
    "  \"That movie was absolutely awful\",\n",
    "  \"The acting was a bit lacking\",\n",
    "  \"The film was creative and surprising\",\n",
    "  \"Absolutely fantastic!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559
    },
    "colab_type": "code",
    "id": "QrZmvZySKQTm",
    "outputId": "aad481ea-c84b-4160-a5e0-bb4aa6a54d10"
   },
   "outputs": [],
   "source": [
    "predictions = getPrediction(pred_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MXkRiEBUqN3n"
   },
   "source": [
    "Voila! We have a sentiment classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "ERkTE8-7oQLZ",
    "outputId": "943b076f-c374-4f2e-be63-990381968d09"
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "predicting_movie_reviews_with_bert_on_tf_hub.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
